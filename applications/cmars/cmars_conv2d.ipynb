{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from cmars_lib.config_mappol import get_config\n",
    "import cmars_lib.mdp_config as mdp_config\n",
    "from cmars_lib.util import *\n",
    "from cmars_lib.cnn import *\n",
    "from cmars_lib.act import *\n",
    "from cmars_lib.mlp import *\n",
    "from cmars_lib.distributions import *\n",
    "from gym import spaces\n",
    "  \n",
    "parser = get_config()\n",
    "parser.add_argument(\"--add_move_state\", action='store_true', default=False)\n",
    "parser.add_argument(\"--add_local_obs\", action='store_true', default=False)\n",
    "parser.add_argument(\"--add_distance_state\", action='store_true', default=False)\n",
    "parser.add_argument(\"--add_enemy_action_state\", action='store_true', default=False)\n",
    "parser.add_argument(\"--add_agent_id\", action='store_true', default=False)\n",
    "parser.add_argument(\"--add_visible_state\", action='store_true', default=False)\n",
    "parser.add_argument(\"--add_xy_state\", action='store_true', default=False)\n",
    "parser.add_argument(\"--use_state_agent\", action='store_true', default=False)\n",
    "parser.add_argument(\"--use_mustalive\", action='store_false', default=True)\n",
    "parser.add_argument(\"--add_center_xy\", action='store_true', default=False)\n",
    "parser.add_argument(\"--use_single_network\", action='store_true', default=False)\n",
    "all_args = parser.parse_known_args()[0]\n",
    "\n",
    "class R_Actor(nn.Module):\n",
    "    def __init__(self, args, obs_space, action_space, device=torch.device(\"cpu\")):\n",
    "        super(R_Actor, self).__init__()\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self._gain = args.gain\n",
    "        self._use_orthogonal = args.use_orthogonal\n",
    "        self._use_policy_active_masks = args.use_policy_active_masks\n",
    "        self._use_naive_recurrent_policy = args.use_naive_recurrent_policy\n",
    "        self._use_recurrent_policy = args.use_recurrent_policy\n",
    "        self._recurrent_N = args.recurrent_N\n",
    "        self.tpdv = dict(dtype=torch.float64, device=device)\n",
    "\n",
    "        obs_shape = get_shape_from_obs_space(obs_space)\n",
    "        base = CNNBase if len(obs_shape) == 3 else MLPBase\n",
    "        self.base = base(args, obs_shape)\n",
    "\n",
    "        self.act = ACTLayer(action_space, self.hidden_size, self._use_orthogonal, self._gain, args)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, obs, rnn_states, masks, available_actions=None, deterministic=False):\n",
    "        obs = check(obs).to(**self.tpdv)\n",
    "\n",
    "        actor_features = self.base(obs)\n",
    "        action_ligits_probs = self.act(actor_features, available_actions, deterministic)\n",
    "\n",
    "        return action_ligits_probs\n",
    "\n",
    "def get_model(layer_count, hidden_size, action_count):\n",
    "    # set configs\n",
    "    assert layer_count in [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    assert hidden_size in [32, 64, 128]\n",
    "    assert action_count in [15, 30, 80, 140]\n",
    "\n",
    "    all_args.layer_N = layer_count\n",
    "    all_args.hidden_size = hidden_size\n",
    "    n_prbs = action_count\n",
    "    models_path = f\"models/output_{action_count}/h{hidden_size}/N{layer_count}/actor_type_embb.pt\"\n",
    "\n",
    "    act_space = spaces.Discrete(n_prbs)\n",
    "    obs_space = spaces.Box(low=0, high=10e6, shape=(mdp_config.EMBB_LOCAL_OBS_VAR_COUNT+mdp_config.AUG_LOCAL_STATE_VAR_COUNT,))\n",
    "\n",
    "    # base policy\n",
    "    device = torch.device('cpu')\n",
    "    model = R_Actor(all_args, obs_space, act_space)\n",
    "    model.load_state_dict(torch.load(models_path, map_location=device))\n",
    "\n",
    "    # remove softmax\n",
    "    class CMARS_Actor_Wrapper(nn.ModuleList):\n",
    "        def __init__(self, model, device=torch.device(\"cpu\")):\n",
    "            super(CMARS_Actor_Wrapper, self).__init__()        \n",
    "            self.to(device)\n",
    "\n",
    "            self.af = nn.ReLU()\n",
    "            self.lin1 = nn.Linear(mdp_config.AUG_LOCAL_STATE_VAR_COUNT + mdp_config.EMBB_LOCAL_OBS_VAR_COUNT, all_args.hidden_size)\n",
    "            self.lin1.weight.data = model.base.mlp.fc1[0].weight.data\n",
    "            self.lin1.bias.data = model.base.mlp.fc1[0].bias.data\n",
    "            \n",
    "            self.midlayers = []\n",
    "            for i in range(all_args.layer_N):\n",
    "                self.midlayers.append(nn.Linear(all_args.hidden_size, all_args.hidden_size))\n",
    "\n",
    "            for iter, item in enumerate(self.midlayers):\n",
    "                item.weight.data = model.base.mlp.fc2[iter][0].weight.data\n",
    "                item.bias.data = model.base.mlp.fc2[iter][0].bias.data\n",
    "\n",
    "            for i in range(all_args.layer_N):\n",
    "                setattr(self, \"lin{}\".format(i+2), self.midlayers[i])\n",
    "\n",
    "            self.out = nn.Linear(all_args.hidden_size, n_prbs)\n",
    "            self.out.weight.data = model.act.action_out.linear.weight.data\n",
    "            self.out.bias.data = model.act.action_out.linear.bias.data\n",
    "\n",
    "        def forward(self, obs):\n",
    "            obs = self.af(self.lin1(obs))\n",
    "\n",
    "            for item in self.midlayers:\n",
    "                obs = self.af(item(obs))    \n",
    "\n",
    "            logits = self.out(obs)\n",
    "\n",
    "            return logits\n",
    "\n",
    "    embb_cmars_wrapper = CMARS_Actor_Wrapper(model)\n",
    "\n",
    "    return embb_cmars_wrapper\n",
    "\n",
    "def get_params_argmax(input_size):\n",
    "    \n",
    "    # Take sum of the input vars\n",
    "    c01 = torch.zeros([1, 1, 1, input_size+1])\n",
    "    c01[0][0][0][0] = 1\n",
    "\n",
    "    c02 = torch.zeros([1, 1, 1, input_size+1])\n",
    "    c02[0][0][0][0] = 1\n",
    "    c02[0][0][0][-1] = 1\n",
    "\n",
    "    return c01, c02\n",
    "\n",
    "def get_plain_comparative_cmars(layer_count=1, hidden_size=32, action_count=15):\n",
    "    class MyModel(nn.ModuleList):\n",
    "        def __init__(self, device=torch.device(\"cpu\")):\n",
    "            super(MyModel, self).__init__()\n",
    "\n",
    "            input_size = 19\n",
    "            self.input_size = input_size\n",
    "            c01, c02 = get_params_argmax(input_size)\n",
    "            \n",
    "            self.ft = torch.nn.Flatten()\n",
    "\n",
    "            #################\n",
    "            # Model\n",
    "            ################# \n",
    "            self.base_model = get_model(layer_count, hidden_size, action_count)\n",
    "            \n",
    "            #################\n",
    "            # Input summation\n",
    "            #################\n",
    "            self.input_conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=[1, input_size+1])\n",
    "            self.input_conv1.weight = torch.nn.Parameter(c01, requires_grad=True)\n",
    "            self.input_conv1.bias = torch.nn.Parameter(torch.zeros_like(self.input_conv1.bias, requires_grad=True))\n",
    "            \n",
    "            self.input_conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=[1, input_size+1])\n",
    "            self.input_conv2.weight = torch.nn.Parameter(c02, requires_grad=True)\n",
    "            self.input_conv2.bias = torch.nn.Parameter(torch.zeros_like(self.input_conv2.bias, requires_grad=True))            \n",
    "            \n",
    "        def forward(self, obs):\n",
    "            # input processing\n",
    "            obs = torch.unsqueeze(obs, 0)\n",
    "            obs = torch.unsqueeze(obs, 0)\n",
    "\n",
    "            input1 = self.input_conv1(obs)\n",
    "            input2 = self.input_conv2(obs)\n",
    "            \n",
    "            input1 = torch.squeeze(input1, 0)\n",
    "            input1 = torch.squeeze(input1, 0)\n",
    "            input2 = torch.squeeze(input2,0)\n",
    "            input2 = torch.squeeze(input2,0)\n",
    "                        \n",
    "            # the model\n",
    "            copy1_logits = self.base_model(input1)\n",
    "            copy2_logits = self.base_model(input2)\n",
    "            \n",
    "            return torch.concat((copy1_logits, copy2_logits), dim=1)\n",
    "\n",
    "    return MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._use_feature_normalization:  False\n",
      "self._use_feature_normalization:  False\n",
      "self._use_feature_normalization:  False\n",
      "self._use_feature_normalization:  False\n",
      "self._use_feature_normalization:  False\n",
      "self._use_feature_normalization:  False\n",
      "self._use_feature_normalization:  False\n",
      "self._use_feature_normalization:  False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "    [[0.1] * 38]\n",
    ")\n",
    "\n",
    "for layer_count in [1, 2]:\n",
    "    for hidden_size in [32, 64]:\n",
    "        for action_count in [15, 30]:\n",
    "            model = get_plain_comparative_cmars(layer_count, hidden_size, action_count)\n",
    "            torch.onnx.export(\n",
    "                model,      # The model being converted\n",
    "                x,          # A dummy input for tracing the model\n",
    "                f\"models/conv2d_based_onnx/model_l{layer_count}_h{hidden_size}_a{action_count}.onnx\", # The output file name for the ONNX model\n",
    "                input_names=['input'],   # Optional: names for the input nodes\n",
    "                output_names=['output'], # Optional: names for the output nodes\n",
    "                opset_version=11         # Optional: specify the ONNX opset version\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_argmax(input_size):\n",
    "    \n",
    "    # Take sum of the input vars\n",
    "    c01 = torch.zeros([1, 1, 1, input_size+1])\n",
    "    c01[0][0][0][0] = 1\n",
    "\n",
    "    c02 = torch.zeros([1, 1, 1, input_size+1])\n",
    "    c02[0][0][0][0] = 1\n",
    "    c02[0][0][0][-1] = 1\n",
    "\n",
    "    return c01, c02\n",
    "\n",
    "\n",
    "class MyModel(nn.ModuleList):\n",
    "        def __init__(self, device=torch.device(\"cpu\")):\n",
    "            super(MyModel, self).__init__()\n",
    "\n",
    "            input_size = 3\n",
    "            self.input_size = input_size\n",
    "            c01, c02 = get_params_argmax(input_size)\n",
    "            \n",
    "            self.ft = torch.nn.Flatten()\n",
    "\n",
    "            self.input_conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=[1, input_size+1])\n",
    "            self.input_conv1.weight = torch.nn.Parameter(c01, requires_grad=True)\n",
    "            self.input_conv1.bias = torch.nn.Parameter(torch.zeros_like(self.input_conv1.bias))\n",
    "\n",
    "\n",
    "            self.input_conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=[1, input_size+1])\n",
    "            self.input_conv2.weight = torch.nn.Parameter(c02, requires_grad=True)\n",
    "            self.input_conv2.bias = torch.nn.Parameter(torch.zeros_like(self.input_conv2.bias))\n",
    "            \n",
    "            self.model = nn.Linear(3, 3)\n",
    "            self.model.weight = torch.nn.Parameter(torch.eye(3))\n",
    "            self.model.bias = torch.nn.Parameter(torch.zeros_like(self.model.bias))\n",
    "\n",
    "        def forward(self, obs):\n",
    "            # input processing\n",
    "            obs = torch.unsqueeze(obs, 0)\n",
    "            obs = torch.unsqueeze(obs, 0)\n",
    "\n",
    "            input1 = self.input_conv1(obs)\n",
    "            input2 = self.input_conv2(obs)\n",
    "            \n",
    "            input1 = torch.squeeze(input1, 0)\n",
    "            input1 = torch.squeeze(input1, 0)\n",
    "            input2 = torch.squeeze(input2,0)\n",
    "            input2 = torch.squeeze(input2,0)\n",
    "\n",
    "            out1 = self.model(input1)\n",
    "            out2 = self.model(input2)\n",
    "            \n",
    "            return torch.concat((out1, out2), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000, 0.1000, 0.2000, 0.2000, 0.2000]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "    [[0.1] * 6]\n",
    ")\n",
    "\n",
    "model = MyModel()\n",
    "print(model(x))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    x,\n",
    "    \"/home/mzi/sys-rl-verif/model_2.onnx\",\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    opset_version=11\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8]) (1,) (0,)\n",
      "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_params_argmax(input_size):\n",
    "    \n",
    "    # Take sum of the input vars\n",
    "    c01 = torch.zeros([1, 1, input_size+1])\n",
    "    c01[0][0][0] = 1\n",
    "\n",
    "    c02 = torch.zeros([1, 1, input_size+1])\n",
    "    c02[0][0][0] = 1\n",
    "    c02[0][0][-1] = 1\n",
    "\n",
    "    return c01, c02\n",
    "\n",
    "c01, c02 = get_params_argmax(7)\n",
    "\n",
    "x = torch.tensor([[[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]])\n",
    "\n",
    "layer = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=8)\n",
    "\n",
    "layer.weight = torch.nn.Parameter(c01)\n",
    "layer.bias = torch.nn.Parameter(torch.zeros_like(layer.bias))\n",
    "\n",
    "print(layer.weight.shape, layer.stride, layer.padding)\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 8]) (1, 1) (0, 0)\n",
      "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "def get_params_argmax(input_size):\n",
    "    \n",
    "    # Take sum of the input vars\n",
    "    c01 = torch.zeros([1, 1, 1, input_size+1])\n",
    "    c01[0][0][0][0] = 1\n",
    "\n",
    "    c02 = torch.zeros([1, 1, 1, input_size+1])\n",
    "    c02[0][0][0][0] = 1\n",
    "    c02[0][0][0][-1] = 1\n",
    "\n",
    "    return c01, c02\n",
    "\n",
    "c01, c02 = get_params_argmax(7)\n",
    "\n",
    "x = torch.tensor([[[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]])\n",
    "\n",
    "layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=[1, 8])\n",
    "\n",
    "layer.weight = torch.nn.Parameter(c02)\n",
    "layer.bias = torch.nn.Parameter(torch.zeros_like(layer.bias))\n",
    "\n",
    "print(layer.weight.shape, layer.stride, layer.padding)\n",
    "print(layer(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crown",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
